[{"slide_number": "1", "title": "The Reversal Curse: LLMs Fail to Learn \"B is A\" from \"A is B\"", "slide_type": "Title Slide", "content": "No Content", "image_desc": "No Image Description", "video_desc": "language model learning", "narration": "The Reversal Curse reveals a surprising limitation in large language models. Despite their impressive abilities LLMs often struggle to reverse simple relationships. This presentation will explore this phenomenon and its implications.", "image_url": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_1.webp", "image_path": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_1.webp", "video_url": "", "video_path": ""}, {"slide_number": "2", "title": "What is the Reversal Curse?", "slide_type": "Image Left", "content": "LLMs trained on \"A is B\" fail to generalize \"B is A\".\nFailure to answer reversed questions correctly.\nLikelihood of correct answer no better than random.", "image_desc": "Question and answer flow chart. Type: diagram", "video_desc": "", "narration": "The Reversal Curse occurs when a language model trained on statements like A is B cannot automatically infer that B is A. This means if a model learns Valentina Tereshkova was the first woman to travel to space it may not correctly answer Who was the first woman to travel to space. The likelihood of the correct answer is often no better than guessing a random name.", "image_url": "https://media.geeksforgeeks.org/wp-content/uploads/20230613154813/Draw-a-flowchart-to-print-the-input-number-5-times.png", "image_path": "data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_2.webp", "video_url": "", "video_path": ""}, {"slide_number": "3", "title": "Example of the Reversal Curse", "slide_type": "Image with Caption", "content": "GPT-4 correctly identifies Tom Cruise's mother but fails to identify Tom Cruise as Mary Lee Pfeiffer's son.", "image_desc": "GPT-4 answering questions about Tom Cruise and Mary Lee Pfeiffer. Type: screenshot", "video_desc": "", "narration": "Consider this example with GPTFour. When asked Who is Tom Cruise's mother it correctly answers Mary Lee Pfeiffer. However when prompted with Who is Mary Lee Pfeiffer's son it struggles to provide the correct answer Tom Cruise. This highlights the Reversal Curse in action.", "image_url": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_3.webp", "image_path": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_3.webp", "video_url": "", "video_path": ""}, {"slide_number": "4", "title": "Why the Reversal Curse Matters", "slide_type": "Image Right", "content": "Failure of basic logical deduction.\nInability to generalize beyond training data.\nLimitation in meta-learning capabilities.", "image_desc": "Brain with question mark. Type: illustration", "video_desc": "", "narration": "The Reversal Curse reveals a fundamental flaw in how LLMs process information. It indicates a failure in logical deduction as A is B logically implies B is A. This limitation also highlights the models' inability to generalize effectively beyond the specific patterns encountered in their training data and their weakness in metalearning.", "image_url": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_4.webp", "image_path": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_4.webp", "video_url": "", "video_path": ""}, {"slide_number": "5", "title": "Experiment 1: Fictitious Celebrities", "slide_type": "Image Left", "content": "Finetuning LLMs on synthetic facts.\nTesting generalization in both directions.\nMeasuring exact-match accuracy and likelihood.", "image_desc": "LLM finetuning process. Type: diagram", "video_desc": "", "narration": "To investigate the Reversal Curse the researchers conducted a series of experiments. In Experiment One they finetuned LLMs on fictitious facts about madeup celebrities. They then tested the models' ability to answer questions in both the original and reversed directions measuring the accuracy and likelihood of correct responses.", "image_url": "https://cdn.prod.website-files.com/614c82ed388d53640613982e/66bb667edb61a05af03b265b_65b7a73f7ee95b6c9e261863_using-prompts-to-fine-tune-llms-with-instruction.webp", "image_path": "data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_5.webp", "video_url": "", "video_path": ""}, {"slide_number": "6", "title": "Experiment 1: Setup", "slide_type": "Image with Caption", "content": "Finetuning on \"Name to Description\" and \"Description to Name\" data, then evaluating in both orders.", "image_desc": "Diagram illustrating finetuning and evaluation process. Type: diagram", "video_desc": "", "narration": "The experiment involved finetuning models on datasets where names preceded descriptions and vice versa. The models were then evaluated on their ability to answer questions in both formats. This setup allowed the researchers to directly assess the impact of the Reversal Curse.", "image_url": "https://miro.medium.com/v2/resize:fit:951/0*R31A71UjHM8R8Pps.png", "image_path": "data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_6.webp", "video_url": "", "video_path": ""}, {"slide_number": "7", "title": "Experiment 1: Results", "slide_type": "Image Right", "content": "High accuracy when the order matches training.\nNear-zero accuracy when the order is reversed.\nNo increase in likelihood for the correct name.", "image_desc": "Graph showing accuracy in same and reversed directions. Type: graph", "video_desc": "", "narration": "The results of Experiment One were striking. The models performed well when the question order matched the training data. However when the order was reversed accuracy plummeted to near zero and the likelihood of the correct name was no higher than random chance.", "image_url": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_7.webp", "image_path": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_7.webp", "video_url": "", "video_path": ""}, {"slide_number": "8", "title": "Experiment 2: Real-World Knowledge", "slide_type": "Image Left", "content": "Testing LLMs on real celebrity-parent relationships.\nComparing accuracy for \"Who is A's parent?\" vs. \"Who is B's child?\".\nEvidence of the Reversal Curse in GPT-4 and Llama models.", "image_desc": "Celebrities with their parents. Type: collage", "video_desc": "", "narration": "Experiment Two shifted focus to realworld knowledge. The researchers tested LLMs on questions about celebrities and their parents. They compared the models' accuracy in answering Who is A's parent versus Who is B's child. The results provided evidence of the Reversal Curse in popular models like GPTFour and Llama.", "image_url": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_8.webp", "image_path": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_8.webp", "video_url": "", "video_path": ""}, {"slide_number": "9", "title": "Experiment 2: Results", "slide_type": "Image Right", "content": "GPT-4: Higher accuracy for parent identification than child identification.\nLlama models: Similar pattern, indicating ordering effect.", "image_desc": "Bar graph comparing parent and child identification accuracy. Type: graph", "video_desc": "", "narration": "The data revealed a clear trend. GPTFour was significantly better at identifying a celebrity's parent than identifying a celebrity when given the parent's name. Llama models exhibited a similar pattern further supporting the presence of the Reversal Curse in realworld knowledge retrieval.", "image_url": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_9.webp", "image_path": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_9.webp", "video_url": "", "video_path": ""}, {"slide_number": "10", "title": "Experiment 3: Reversing Instructions", "slide_type": "Image Left", "content": "Finetuning on question-answer pairs as instructions.\nTesting generalization from instructions to examples.\nReinforcing the Reversal Curse phenomenon.", "image_desc": "Question and answer pair. Type: illustration", "video_desc": "", "narration": "In Experiment Three the researchers explored the Reversal Curse in the context of instructions. They finetuned LLMs on questionanswer pairs presented as instructions. The models were then tested on their ability to generalize from these instructions to standard questionanswer examples once again highlighting the Reversal Curse.", "image_url": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_10.webp", "image_path": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_10.webp", "video_url": "", "video_path": ""}, {"slide_number": "11", "title": "Related Work", "slide_type": "Two Columns", "content": "**Influence Functions**\nGrosse et al. (2023) show training examples with the correct order are more influential.\nConsistent with the Reversal Curse.\n**Knowledge Editing**\nMeng et al. (2023) find model editing is not bidirectional.\nSuggests LLMs store associations differently depending on direction.", "image_desc": "List of research paper citations. Type: text", "video_desc": "", "narration": "Several related works support the findings of this research. Studies using influence functions demonstrate that training examples with the correct order have a greater impact on model outputs. Research on knowledge editing also suggests that LLMs store associations in a directional manner consistent with the Reversal Curse.", "image_url": "https://www.fhnw.ch/plattformen/academicguide/wp-content/uploads/sites/271/Screenshot-2023-10-31-at-15.37.07-1024x578.png", "image_path": "data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_11.webp", "video_url": "", "video_path": ""}, {"slide_number": "12", "title": "Conclusion and Future Work", "slide_type": "Image with Caption", "content": "The Reversal Curse highlights a fundamental limitation in LLMs' ability to generalize knowledge. Future work should explore the underlying causes and potential solutions.", "image_desc": "Circuit board with question marks. Type: illustration", "video_desc": "", "narration": "In conclusion the Reversal Curse exposes a significant weakness in the generalization abilities of large language models. Further research is needed to understand the underlying mechanisms driving this phenomenon and to develop strategies for mitigating its effects. Future work includes studying other types of relations finding reversal failures via entitylinking and analyzing the practical impact of the Reversal Curse.", "image_url": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_12.webp", "image_path": "/data/videos/6db4bd0a-3945-4b5a-b3dd-e18408471115/images/image_12.webp", "video_url": "", "video_path": ""}]